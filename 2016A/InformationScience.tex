%% LyX 2.2.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[oneside,english]{book}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[a5paper]{geometry}
\geometry{verbose,tmargin=2cm,bmargin=2cm,lmargin=1cm,rmargin=1cm}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{textcomp}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\PassOptionsToPackage{normalem}{ulem}
\usepackage{ulem}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage[dvipdfmx]{hyperref}
\usepackage[dvipdfmx]{pxjahyper}

\makeatother

\usepackage{babel}
\begin{document}

\title{2016-A 情報通信理論}

\author{教員: 相澤 入力: 高橋光輝}

\maketitle
\global\long\def\pd#1#2{\frac{\partial#1}{\partial#2}}
\global\long\def\d#1#2{\frac{\mathrm{d}#1}{\mathrm{d}#2}}
\global\long\def\pdd#1#2{\frac{\partial^{2}#1}{\partial#2^{2}}}
\global\long\def\dd#1#2{\frac{\mathrm{d}^{2}#1}{\mathrm{d}#2^{2}}}
\global\long\def\e{\mathrm{e}}
\global\long\def\i{\mathrm{i}}
\global\long\def\j{\mathrm{j}}
\global\long\def\grad{\operatorname{grad}}
\global\long\def\rot{\operatorname{rot}}
\global\long\def\div{\operatorname{div}}
\global\long\def\diag{\operatorname{diag}}
\global\long\def\rank{\operatorname{rank}}
\global\long\def\prob{\operatorname{Prob}}
\global\long\def\cov{\operatorname{Cov}}
\global\long\def\when#1{\left.#1\right|}


\section*{第1回}

\paragraph{本講義の内容}
\begin{itemize}
\item 情報量

\begin{itemize}
\item エントロピー
\item 相互情報量
\end{itemize}
\item 情報源符号化
\item 通信路符号化
\item 通信システム

\begin{center}
\includegraphics{images/InformationScience/1-1}
\par\end{center}
\item 実際の通信システム

\begin{center}
\includegraphics{images/InformationScience/1-2}
\par\end{center}
\item 情報源符号化

\begin{itemize}
\item 平均符号長をなるべく短くする。
\item どこまで短くできるか。
\end{itemize}
\item 通信路符号化

\begin{itemize}
\item 誤り訂正・検出
\item どこまで正しく送れるのか?
\end{itemize}
\item 情報をどう測るか?

\begin{center}
\includegraphics{images/InformationScience/1-3}
\par\end{center}
\begin{itemize}
\item 統計的な定義～発生確率の関係

\begin{enumerate}
\item まれなでき事が起きる→情報量大
\item 独立なでき事 1, 2 が同時に起こる→$I_{1}+I_{2}$
\end{enumerate}
\item でき事 1, 2 に対して、$P_{1},P_{2}$を発生確率とすると、

\begin{enumerate}
\item $P_{1}<P_{2}\rightarrow I\left(P_{1}\right)>I\left(P_{2}\right)$
\item $I\left(P_{1}P_{2}\right)=I\left(P_{1}\right)+I\left(P_{2}\right)$
\end{enumerate}
\end{itemize}
\end{itemize}

\paragraph{定義}

事象$a$の発生確率が$P\left(a\right)$であるとき、その事象$a$の(自己)情報量は
\[
I\left(a\right)=-\log_{2}P\left(a\right)\left(\text{ビット/シャノン}\right)
\]

例えばある日の天気が晴れである確率が0.6であるとき、その情報量は0.73、雨である確率が0.4であるとき、その情報量は1.32である。

\paragraph{定義 情報源の情報量}

複数の事象の集まりに対する情報量→平均情報量

$a_{1},\cdots,a_{n}$の$M$個の``独立な''事象であるとすると、情報源全体の情報量は
\begin{align*}
H & =-P\left(a_{1}\right)\log P\left(a_{1}\right)-P\left(a_{2}\right)\log P\left(a_{2}\right)\cdots\\
 & =-\sum_{i=1}^{M}P\left(a_{i}\right)\log P\left(a_{i}\right)
\end{align*}

\begin{itemize}
\item 独立な$M$個であれば、平均情報量が情報源のエントロピー(正の情報量)
\item 独立でないとき?

\begin{itemize}
\item →シンボルの$m$次拡大
\item →拡大した情報源に対して平均情報源
\end{itemize}
\end{itemize}

\paragraph{定義 情報源の情報量}

複数の事象の集まりに対する情報量
\begin{center}
\includegraphics{images/InformationScience/1-4}
\par\end{center}

\begin{align*}
H\left(A_{1}\right) & =-0.8\log0.8-0.15\log0.15-0.05\log0.05\\
 & =0.88\left(\text{ビット}\right)\\
H\left(A_{2}\right) & =1.58\left(\text{ビット}\right)
\end{align*}


\paragraph{エントロピーの性質}
\begin{enumerate}
\item 偏りが大きいとエントロピーが小さくなり、偏りが大きいとエントロピーは小さくなる。
\item $H\left(A\right)\geqq0$
\item $H\left(A\right)\leqq-\sum_{t=1}^{M}\frac{1}{M}\log\frac{1}{M}=\log M$
\end{enumerate}
\[
\because\begin{cases}
H=-\sum P_{i}\log P_{i}\\
\sum P_{i}=1
\end{cases}
\]
を条件に最大化し、
\[
L=H-\lambda\left(\sum P_{i}-1\right)
\]
\[
\begin{cases}
\pd L{P_{1}}=\pd L{P_{2}}=\cdots=0\\
\pd L{\lambda}=0
\end{cases}
\]
\[
\pd L{P_{i}}=-\log P_{i}-\log\e-\lambda=0
\]
\[
\pd L{\lambda}=\sum P_{i}-1=0
\]

\begin{align*}
\log P_{i} & =-\log\e-\lambda\\
P_{i} & =\frac{1}{\e2^{\lambda}}\\
 & =c
\end{align*}
とすると、
\[
c=\frac{1}{M}
\]


\paragraph{エントロピー関数}
\begin{center}
\includegraphics{images/InformationScience/1-5}
\par\end{center}

\[
H=-p\log p-\left(1-p\right)\log\left(1-p\right)
\]

\begin{center}
\includegraphics{images/InformationScience/1-6}
\par\end{center}

\paragraph{曖昧さとエントロピー}

エントロピー 曖昧さの変化の度合い
\begin{itemize}
\item あいまい
\item →シグナルを受けた時点で曖昧さが消える
\item →あいまい
\end{itemize}

\section*{第2回}

\paragraph{情報量}
\begin{itemize}
\item 自己情報量 $-\log p_{i}$: でき事
\item 平均情報量 $-\sum p_{i}\log p_{i}$: 情報源

\begin{itemize}
\item 無記憶情報源
\end{itemize}
\item あいまいさとエントロピー
\end{itemize}

\paragraph{エントロピーと代表的系列}
\begin{center}
\includegraphics{images/InformationScience/2-1}
\par\end{center}

$n$が十分大とすると、
\begin{align*}
\frac{n_{i}}{n} & \simeq p_{i}\\
n_{i} & \simeq np_{i}
\end{align*}

ある系列$\sigma$に対し、
\begin{align*}
P\left(\sigma\right) & =p^{n_{1}}p^{n_{2}}\cdots p^{n_{M}}\\
 & =\prod_{i=1}^{M}p^{n_{i}}\\
 & =\prod p^{np_{i}}\\
p_{i} & =2^{\log p_{i}}\\
P\left(\sigma\right) & =\prod2^{np_{i}\log p_{i}}\\
 & =2^{n\sum p_{i}\log p_{i}}\\
 & =2^{-nH\left(A\right)}\\
 & =\frac{1}{2^{nH\left(A\right)}}
\end{align*}


\paragraph{相互情報量}

・通信的立場
\begin{center}
\includegraphics{images/InformationScience/2-3}
\par\end{center}

通信による情報源の情報量\\
=$H\left(A\right)$－誤りによる混乱\\
=(情報を受け取る以前の曖昧さ)－(通信を受け取った以降に残る曖昧さ=誤りに対する減少分)

・汎用的立場

複合事象のエントロピー
\begin{center}
\includegraphics{images/InformationScience/2-4}
\par\end{center}

$B$を観測→どれだけ$A$の情報量を受け取ることができるか

$B$を介して知ることのできる情報源の情報量

\paragraph{例}
\begin{center}
\includegraphics{images/InformationScience/2-5}
\par\end{center}

\begin{align*}
H\left(A\right) & =-0.65\log0.65-0.35\log0.35\\
 & =0.93\\
H\left(B\right) & =-0.60\log0.60-0.4\log0.4\\
 & =0.97\\
H\left(AB\right) & =-\sum P\left(a_{i}b_{i}\right)\log P\left(a_{i}b_{i}\right)\\
 & =-0.55\log0.55\cdots-0.30\log0.30\\
 & =1.54
\end{align*}

\[
H\left(A\right),H\left(B\right)<H\left(AB\right)<H\left(A\right)+H\left(B\right)
\]

$B$を知ってのちの$A$の曖昧さ(条件付きエントロピー)

\begin{align*}
 & H\left(A|b_{1}\right)\\
 & =-\sum P\left(a_{i}|b_{i}\right)\log P\left(a_{i}|b_{i}\right)\\
 & =-\frac{55}{60}\log\frac{55}{60}-\frac{5}{60}\log\frac{5}{60}\\
 & =0.41
\end{align*}

\[
H\left(A|b_{2}\right)=\cdots-0.81
\]

\begin{align*}
H\left(A|B\right) & =-\sum P\left(b_{i}\right)H\left(A|b_{j}\right)\\
 & =-\sum_{i}P\left(b_{i}\right)\sum P\left(a_{j}|b_{i}\right)\log P\left(a_{j}b_{i}\right)\\
 & =0.57
\end{align*}

\[
I\left(A,B\right)=H\left(A\right)-H\left(A|B\right)=0.36
\]

$I\left(A,B\right)$: $B$を知ることで分かる$A$の情報量 相互情報量

\begin{align*}
I\left(A,B\right)= & -\sum_{i}P\left(a_{i}\right)\log P\left(a_{i}\right)\\
 & +\sum_{i}P\left(b_{j}\right)\sum_{i}P\left(a_{i}|b_{j}\right)\log P\left(a_{i}|b_{j}\right)\\
= & \sum_{i}\sum_{j}P\left(a_{i}b_{j}\right)\log\frac{P\left(a_{i}|b_{j}\right)}{P\left(a_{i}\right)P\left(b_{j}\right)}
\end{align*}

$B$から得る$A$の情報量=$A$から得る$B$の情報量
\begin{center}
\includegraphics{images/InformationScience/2-6}
\par\end{center}

\paragraph{相互情報量の性質}

\[
0\leq I\left(A,B\right)\leq H\left(A\right)
\]

\[
0\leq H\left(A,B\right)\leq H\left(A\right)
\]


\paragraph{例: 天気予報}
\begin{center}
\includegraphics{images/InformationScience/2-7}
\par\end{center}

\[
H\left(A\right)=1\mathrm{bit}
\]

\begin{align*}
I\left(A,B\right) & =\sum_{i}\sum_{j}P\left(a_{i}b_{j}\right)\log\frac{P\left(a_{i}b_{j}\right)}{P\left(a_{i}\right)P\left(b_{j}\right)}\\
 & =1+p\log p+\left(1-p\right)\log\left(1-p\right)
\end{align*}

あるいは
\begin{align*}
H\left(A|B\right) & =\underbrace{-p\log p-\left(1-p\right)\log\left(1-p\right)}_{\text{エントロピー関数}}\\
I\left(A,B\right) & =H\left(A\right)-H\left(A|B\right)
\end{align*}

\begin{center}
\includegraphics{images/InformationScience/2-8}
\par\end{center}

\begin{align*}
I\left(A,B\right) & =H\left(A\right)-H\left(A|B\right)\\
 & =H\left(B\right)-H\left(B|A\right)\\
 & =H\left(A\right)+H\left(B\right)-H\left(AB\right)
\end{align*}

\begin{center}
\includegraphics{images/InformationScience/2-9}
\par\end{center}

\section*{第3回}

\paragraph{相互情報量}

情報源符号化(情報圧縮)
\begin{itemize}
\item 符号の望ましい性質
\item どのように符号化?
\item どこまで圧縮できるか?
\end{itemize}

\paragraph{望ましい性質}

例:
\begin{center}
\includegraphics{images/InformationScience/3-1}
\par\end{center}

平均符号長 $L=\sum p_{i}l_{i}$

$C_{5}$:
\begin{center}
\includegraphics{images/InformationScience/3-2}
\par\end{center}
\begin{itemize}
\item 一意復号可能
\begin{itemize}
\item 瞬時符号
\begin{itemize}
\item 等長符号
\item 非等長符号
\end{itemize}
\item 非瞬時符号
\end{itemize}
\item 一意復号不可能 ($C_{5}$)
\end{itemize}
\begin{center}
\includegraphics{images/InformationScience/3-3}
\par\end{center}

\paragraph{瞬時符号の条件}

語頭条件 (Prefix rule): ある符号が別の符号の語頭になっていない (必要十分条件)

\paragraph{符号の木}
\begin{center}
\includegraphics{images/InformationScience/3-4}
\par\end{center}

瞬時符号→符号の木の葉にだけ符号がある

\paragraph{クラフトの不等式}

符号の``長さ''に関する条件

長さ$l_{1},l_{2},\cdots,l_{m}$となる瞬時符号が存在する必要十分条件

\[
2^{-l_{1}}+2^{-l_{2}}+\cdots+2^{-l_{m}}\le1
\]


\paragraph{平均符号長の条件}

どこまで短くできるか?

\paragraph{情報源符号化定理}

平均符号長$L$は、情報源のエントロピー$H\left(S\right)$に限りなく近づけることができる。

→
\[
H\left(S\right)\le L\le H\left(S\right)+\varepsilon
\]

\begin{center}
\includegraphics{images/InformationScience/3-5}
\par\end{center}

\[
H\left(S\right)=-\sum p_{i}\log p_{i}
\]

\[
L=\sum p_{i}l_{i}
\]


\paragraph{定理}
\begin{enumerate}
\item $L<H\left(S\right)+1$となる瞬時符号を作ることができる。
\item $H\left(S\right)\le L$でなければならない
\end{enumerate}
→$H\left(S\right)\le L<H\left(S\right)+1$

\paragraph{1. について}

符号長$l_{i}$について
\[
-\log p_{i}\le l_{i}<-\log_{i}+1
\]
と$l_{i}$を選ぶ。

\[
\log p_{i}\ge-l_{i}
\]

\[
p_{i}\ge2^{-l_{i}}
\]

\[
\sum p_{i}\ge\sum2^{-l_{i}}
\]

\[
1\ge\sum2^{-l_{i}}
\]

クラフトの不等式

瞬時符号が存在する。

\paragraph{シャノンの補助定理}

$q_{1},\cdots'q_{m}$を非負の数で$q_{1}+\cdots+q_{m}=1$とする。

\[
-p_{i}\log q_{i}\ge-p_{i}\log p_{i}
\]

等号は$q_{i}=p_{i}$のとき成立(シャノンの補助定理)。

$l_{i}=-\log q_{i}$と$q_{i}$を選ぶ。

$l_{i}$瞬時符号があれば
\[
\sum2^{-l_{i}}\le1
\]


\paragraph{どうやって最短符号を作るか?}

ハフマン符号: 平均符号長を最短化する符号
\begin{center}
\includegraphics{images/InformationScience/3-6}
\par\end{center}

\section*{第4回}
\begin{itemize}
\item ハフマン符号
\item $H\leqq L<H+1$
\end{itemize}

\paragraph{情報源の拡大}

例:

図情4-1

\[
L=1
\]

\begin{align*}
H & =-0.8\log0.8-0.2\log0.2\\
 & =0.72
\end{align*}

2次の拡大情報源 ハフマン符号

図情4-2

平均符号長

\begin{align*}
L & =0.64\times1+0.16\times2+0.16\times3+0.04\times3\\
 & =1.56
\end{align*}

もともとの1シンボルあたり

\[
\frac{L}{2}=0.78
\]

$n$次の拡大情報源に対して、
\[
H\left(S^{n}\right)\leq L_{n}<H\left(S^{n}\right)+1
\]

1シンボルあたり、
\[
\frac{H\left(S^{n}\right)}{n}\leq\frac{L_{n}}{n}<\frac{H\left(S^{n}\right)}{n}+\frac{1}{n}
\]


\paragraph{無記憶情報源}

\[
H\left(S^{n}\right)=nH\left(S\right)
\]

\[
\rightarrow-\sum_{x0}\cdots\sum_{x_{n-1}}P\left(x_{0}\cdots x_{n-1}\right)\log P\left(x_{0}\cdots x_{n-1}\right)
\]

\[
\rightarrow-\sum_{x0}\cdots\sum_{x_{n-1}}P\left(x_{0}\right)P\left(x_{1}\right)\cdots P\left(x_{n-1}\right)\left\{ \log P\left(x_{0}\right)+\log P\left(x_{1}\right)+\cdots+\log P\left(x_{n-1}\right)\right\} 
\]

\[
\rightarrow-\sum P\left(x_{0}\right)\log P\left(x_{0}\right)-\sum P\left(x_{1}\right)\log P\left(x_{1}\right)-\cdots-\sum P\left(x_{n-1}\right)\log P\left(x_{n-1}\right)
\]
無記憶情報源の場合
\[
H\left(S\right)\leq L<H\left(S\right)+\frac{1}{n}
\]

$n\rightarrow\text{大}$とすると、$L\rightarrow H\left(S\right)$

$H\left(S\right)$の計算:
\begin{itemize}
\item $P\left(x\right)$求めて定義
\item $P\left(x\right)$求めて符号化、拡大した$L$
\end{itemize}

\paragraph{記憶のある情報源}

$a_{0},a_{0},a_{0},a_{1},a_{1},a_{1}\cdots$

\[
H\left(S^{n}\right)\leq L<H\left(S^{n}\right)-1
\]

\[
\frac{H\left(S^{n}\right)}{n}\leq\frac{L_{n}}{n}<\frac{H\left(S^{n}\right)}{n}+\frac{1}{n}
\]

記憶のある情報源のエントロピー
\[
\frac{H\left(S\right)}{n}
\]
は十分大きな$n$次の拡大情報源の平均情報量である。

\[
\underbrace{a_{0}a_{0}a_{1}\cdots a_{1}}_{A_{i}}\underbrace{a_{1}a_{0}\cdots a_{0}}_{A_{j}}
\]

$n$が大きければ無記憶に近づく。

→平均情報量

→エントロピー

\paragraph{情報源符号化定理}

情報源$S$は平均符号長$L$が
\[
H\left(S\right)\leq L<H\left(S\right)+\varepsilon
\]
($\varepsilon$は十分小さくなる)となるように符号化できる。

($L$を$H\left(S\right)$に限りなく近づけることができる。)

\paragraph{例}

アルファベットの例

(0次エントロピー、全て均一) $H\left(S\right)=\log2.7=4.75$

(一次エントロピー、単一文言の生成確率) 4.03

(二次エントロピー$aa,ab,\cdots$の生起確率) 3.32

\paragraph{例 2値画像}

図情4-3

\paragraph{マルコフ情報源}

記憶のある情報源の一例のエントロピー

単一マルコフ情報源: 一つ直前のシンボルの影響を受ける。

無記憶情報源だと思い
\[
H_{1}\left(S\right)=-\frac{2}{3}\log\frac{2}{3}-\frac{1}{3}\log\frac{1}{3}=0.918
\]

しかしながら、記憶のある情報源なので、
\[
\frac{H_{1}\left(S^{n}\right)}{n}\rightarrow?
\]


\section*{第5回}
\begin{itemize}
\item ハフマン符号
\item 情報源拡大 (ブロック化)
\item マルコフ情報源
\item 情報システム符号化
\begin{itemize}
\item 記憶のなし/ある
\end{itemize}
\item よりよい符号化法
\begin{itemize}
\item ハフマン符号+ブロック化で十分に効率を上げられる
\item $n=100$ 元々のシンボルが2つだとしても、テーブルの大きさ$2^{100}\simeq10^{30}$
\end{itemize}
\end{itemize}

\paragraph{ランレングス符号化}

run/length: 連なり/符号化

図情5-1

図情5-2

\paragraph{ランレングス+ハフマン符号化}

図情5-3

根元を 平均ラン長$L_{R}=\sum p_{i}i$とすると、元々の1シンボルあたりでは、(正確ではないが)
\[
\frac{H}{L_{R}}<\frac{B}{L_{R}}<\frac{H}{L_{R}}+\frac{1}{L_{R}}
\]

\[
H\leq B<H+1
\]

\begin{align*}
H & =\left(\text{ラン長のエントロピー}\right)\\
 & =-\sum p_{i}\log p_{i}
\end{align*}


\paragraph{ファクシミリ符号化}

ラインスキャンした画像データをファクシミリ符号化により圧縮

標準解像度 3.85ライン/mm

高解像度 7.7ライン/mm

A4 1枚 (210mmx297mm) →2Mビット

\paragraph{MM・MR符号}

MH (Modified Huffman)

MR (Modified Read)

\paragraph{MH符号}

ランレングス$l$

\[
l=m+64n\:0\leq m\leq63
\]

\[
\left\{ \overbrace{0,1,2,\cdots,63}^{\text{terminating符号}},\overbrace{64,128,\cdots,1728}^{\text{makeup符号}},\text{EOL}\right\} 
\]

→2元シンボルに対して符号の設計

\[
\text{白の150のラン}=W_{150}=W_{128}W_{22}=\text{10010 0000011}
\]

EOL: End of Life

\paragraph{MR符号}
\begin{itemize}
\item ライン間の相関の利用
\item 参照ラインに対しての変化で符号化
\item 同期的にMH符号
\begin{itemize}
\item 標準 2ラインごと
\item 高解像度 4ラインごと
\end{itemize}
\end{itemize}

\section*{第6回}

\paragraph{算術符号}

``エリアス符号''

図情6-1

\[
P\left(0\right)=0.8,P\left(1\right)=0.2
\]

確率を反映させた``区切り''を用いる

図情6-2

図情6-3

実際に長さ$N$の系列$S$にて、

\[
P_{0}\cdots0--N_{02}
\]

\[
P_{1}\cdots1--N_{12}
\]

$P_{0}^{N_{0}}\times P_{1}^{N_{1}}$の区間内の点を求めることになる。

$P_{0}^{N_{0}}\times P_{1}^{N_{1}}$の点を表すのに必要な精度(ケタ数)
\[
-\log_{2}P_{0}^{N_{0}}\times P_{1}^{N_{1}}=-N_{0}\log_{2}P_{0}-N_{1}\log_{2}P_{1}
\]

1シンボル当たりを求めると、

\[
-\frac{N_{0}}{N}\log P_{0}-\frac{N_{1}}{N}\log P_{1}\Rightarrow-P_{0}\log P_{0}-P_{1}\log P_{1}
\]


\paragraph{算術符号}
\begin{itemize}
\item 有限語長
\item 劣性確率
\end{itemize}
$a_{2}a_{1}a_{1}$ (長さは3まで)

$\left\{ a_{1},a_{2}\right\} $ $P\left(a_{1}\right)=0.75,P\left(a_{2}\right)=0.25$

図情6-4
\begin{itemize}
\item テーブルを使わない
\item 算術演算で符号を生成
\end{itemize}

\paragraph{通信路符号化}
\begin{itemize}
\item 通信路の誤りを訂正(検出)
\item 誤り訂正の限界
\item 通信路モデルについて
\begin{itemize}
\item 記憶のない(定常的な)通信路 (ランダムに誤りが起こる通信路)
\end{itemize}
\end{itemize}
図情6-5

\[
\left[\begin{array}{cccc}
P\left(b_{1}|a_{1}\right) & \cdots & \cdots & P\left(b_{n}|a_{n}\right)\\
 & \ddots\\
 &  & P\left(b_{j}|a_{i}\right)\\
 &  &  & \ddots
\end{array}\right]\triangleq\text{通信路行列}
\]

\[
P\left(b_{j}|a_{i}\right)=P_{ji}
\]


\paragraph{典型的な通信路}
\begin{enumerate}
\item 2元対称通信路

図情6-6

通信路行列
\[
\left[\begin{array}{cc}
1-p & p\\
p & 1-p
\end{array}\right]
\]

\item 2元対称消失通信路

図情6-7

\[
\left[\begin{array}{ccc}
1-p-q & p & q\\
p & 1-p-q & q
\end{array}\right]
\]

\end{enumerate}

\paragraph{2元対称通信路}

誤り源を用いて通信路を表すこともできる。

\paragraph{通信路容量 (Channel Capacity)}

通信路で送ることのできる情報量の\uline{最大値}

\[
C\triangleq\max I\left(A,B\right)
\]

図情6-8

\begin{align*}
I\left(A,B\right) & =H\left(A\right)-H\left(A|B\right)\\
 & =H\left(B\right)-H\left(B|A\right)\\
 & =H\left(A\right)+H\left(B\right)-H\left(AB\right)
\end{align*}

\begin{align*}
I\left(A,B\right) & =\sum P_{C}\left(a_{i},b_{j}\right)\log\frac{P_{C}\left(a_{i},b_{j}\right)}{P_{X}\left(a_{i}\right)P_{Y}\left(b_{j}\right)}\\
 & =\sum_{i}\sum_{j}P\left(a_{i}\right)P\left(b_{j}|a_{i}\right)\log\frac{P\left(a_{i}\right)P\left(b_{j}|a_{i}\right)}{P\left(a_{i}\right)P\left(b_{j}\right)}\\
 & =\sum_{i}P\left(a_{i}\right)\sum P\left(b_{j}|a_{i}\right)\log\frac{P\left(b_{j}|a_{i}\right)}{P\left(b_{j}\right)}\\
 & =H\left(B\right)+p\log P+\left(a-p\right)\log\left(1-P\right)
\end{align*}

\[
C=1+p\log P+\left(1-p\right)\log\left(1-P\right)
\]

図情6-9

\paragraph{加法的通信路}

図情6-10

\begin{align*}
I\left(X,Y\right) & =H\left(Y\right)-H\left(Y|X\right)\\
 & =H\left(Y\right)-H\left(X\oplus E|X\right)\\
 & =H\left(Y\right)-H\left(E\right)\\
 & =H\left(Y\right)+p\log P+\left(1-p\right)\log\left(1-P\right)
\end{align*}


\section*{第7回}

一部紛失

\paragraph{通信路符号}
\begin{itemize}
\item 通信路の誤りに耐性を持たせる→誤り訂正、誤り検出
\item ``冗長度''を付け加える
\end{itemize}

\paragraph{多数決符号}
\begin{itemize}
\item $0\rightarrow000$ (符号語)
\item $1\rightarrow111$ (符号語)
\end{itemize}
誤りが1つまでなら誤り訂正可能

\paragraph{受信符号空間}

\[
\left\{ 000,001,\cdots,111\right\} 
\]

\begin{itemize}
\item 符号語 $w_{i}$～符号語数$M$→$\log_{2}M$ (1)→符号にして$n$(3)
\item 受信符号空間
\item 復号領域 $\Omega_{i}$
\item 伝送速度 $R$
\item 冗長度 $1-R\left(\frac{2}{3}\right)$
\item 伝送速度・伝送効率 $\frac{\log_{2}M}{n}\frac{1}{3}=R$
\end{itemize}

\paragraph{通信路が与えられた時、伝承億度の上限は?}

図情7-1

\paragraph{通信路符号化定理}

$R<C$(通信路容量)であれば、限りなく復号誤り率を低くする符号が存在する。$R>C$では、そうできない。

\paragraph{ランダム符号化法による証明}
\begin{enumerate}
\item 通信路容量$C$、$C$を実現するときの情報源$S_{0}$
\item 伝送速度$R$
\item $S_{0}$から発生する長さ$n$の系列をランダムに$M=2^{nR}$とり符号とする。$C_{0}=\left\{ w_{1},w_{2},\cdots,w_{M}\right\} $
\end{enumerate}
図情7-2

図情7-3
\begin{itemize}
\item $S_{0}$の長さ$n$の代表的系列$2^{nH\left(X\right)}$
\item $M\left(=2^{nR}\right)$個の符号語を選ぶ
\item $S_{0}$の系列が符号になる確率$\frac{2^{nR}}{2^{nH\left(X\right)}}$

\[
C=H\left(X\right)-H\left(X|Y\right)
\]

\item 通信路を介した場合の条件付きエントロピー$H\left(X|Y\right)$に対応する代表的系列数
\item 正しく伝送できるためには、$2^{nH\left(X|Y\right)}$にひとつだけ符号語を選んでいれば良い。
\item $2^{nH\left(X|Y\right)}$が唯一の符号語を含む確率$P$は、
\begin{align*}
P & =\left(1-\frac{2^{nR}}{2^{nH\left(X\right)}}\right)^{2^{nH\left(X|Y\right)-1}}\\
 & \simeq1-2^{H\left(X|Y\right)}2^{nR-nH\left(X\right)}
\end{align*}

\begin{align*}
P & =1-2^{-n\left(H\left(X\right)-H\left(X|Y\right)-R\right)}\\
 & =1-2^{-n\left(C-R\right)}
\end{align*}

$C>R$であれば、$n$→十分大きく$P\rightarrow1$

$C<R$であれば、そうはならない。
\end{itemize}

\paragraph{通信路符号化法}

図情7-4

\paragraph{ハミング距離}

符号間の距離の尺度

\begin{align*}
d_{H}\left(001,000\right) & =1\\
d_{H}\left(111,000\right) & =3
\end{align*}


\paragraph{ハミング重み}

\begin{align*}
w_{H}\left(001\right) & =1\\
w_{H}\left(111\right) & =3
\end{align*}

\[
d_{H}\left(a,b\right)=w_{H}\left(a-b\right)
\]
←2元加減算

\paragraph{$t$重誤り訂正}

図情7-5
\begin{itemize}
\item すべての符号語間の距離$\geq2t+1$ 
\item 最小の符号語距離$=2t+1$
\end{itemize}
$t$重の誤りに対して、符号語が他の符号語の復号領域に入らない。

復号領域が重ならない。

\paragraph{$t$重誤り検出}

$t$重誤りが起こっても、他の符号語にはならない。

図情7-6
\begin{itemize}
\item すべての符号語間距離$\geq t+1$
\item 最小符号語間距離$=t+1$
\end{itemize}

\paragraph{符号の構成法}
\begin{itemize}
\item 線形符号
\item 巡回符号
\item BCH, RS⋯
\item 畳み込み符号
\end{itemize}

\paragraph{次回休講}
\end{document}
