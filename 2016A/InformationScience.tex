%% LyX 2.2.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[oneside,english]{book}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[a5paper]{geometry}
\geometry{verbose,tmargin=2cm,bmargin=2cm,lmargin=1cm,rmargin=1cm}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{textcomp}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage[dvipdfmx]{hyperref}
\usepackage[dvipdfmx]{pxjahyper}

\makeatother

\usepackage{babel}
\begin{document}

\title{2016-A 情報通信理論}

\author{教員: 相澤 入力: 高橋光輝}

\maketitle
\global\long\def\pd#1#2{\frac{\partial#1}{\partial#2}}
\global\long\def\d#1#2{\frac{\mathrm{d}#1}{\mathrm{d}#2}}
\global\long\def\pdd#1#2{\frac{\partial^{2}#1}{\partial#2^{2}}}
\global\long\def\dd#1#2{\frac{\mathrm{d}^{2}#1}{\mathrm{d}#2^{2}}}
\global\long\def\e{\mathrm{e}}
\global\long\def\i{\mathrm{i}}
\global\long\def\j{\mathrm{j}}
\global\long\def\grad{\mathrm{grad}}
\global\long\def\rot{\mathrm{rot}}
\global\long\def\div{\mathrm{div}}
\global\long\def\diag{\mathrm{diag}}
\global\long\def\when#1{\left.#1\right|}


\section*{第1回}

\paragraph{本講義の内容}
\begin{itemize}
\item 情報量

\begin{itemize}
\item エントロピー
\item 相互情報量
\end{itemize}
\item 情報源符号化
\item 通信路符号化
\item 通信システム

\begin{center}
\includegraphics{images/InformationScience/1-1}
\par\end{center}
\item 実際の通信システム

\begin{center}
\includegraphics{images/InformationScience/1-2}
\par\end{center}
\item 情報源符号化

\begin{itemize}
\item 平均符号長をなるべく短くする。
\item どこまで短くできるか。
\end{itemize}
\item 通信路符号化

\begin{itemize}
\item 誤り訂正・検出
\item どこまで正しく送れるのか?
\end{itemize}
\item 情報をどう測るか?

\begin{center}
\includegraphics{images/InformationScience/1-3}
\par\end{center}
\begin{itemize}
\item 統計的な定義～発生確率の関係

\begin{enumerate}
\item まれなでき事が起きる→情報量大
\item 独立なでき事 1, 2 が同時に起こる→$I_{1}+I_{2}$
\end{enumerate}
\item でき事 1, 2 に対して、$P_{1},P_{2}$を発生確率とすると、

\begin{enumerate}
\item $P_{1}<P_{2}\rightarrow I\left(P_{1}\right)>I\left(P_{2}\right)$
\item $I\left(P_{1}P_{2}\right)=I\left(P_{1}\right)+I\left(P_{2}\right)$
\end{enumerate}
\end{itemize}
\end{itemize}

\paragraph{定義}

事象$a$の発生確率が$P\left(a\right)$であるとき、その事象$a$の(自己)情報量は
\[
I\left(a\right)=-\log_{2}P\left(a\right)\left(\text{ビット/シャノン}\right)
\]

例えばある日の天気が晴れである確率が0.6であるとき、その情報量は0.73、雨である確率が0.4であるとき、その情報量は1.32である。

\paragraph{定義 情報源の情報量}

複数の事象の集まりに対する情報量→平均情報量

$a_{1},\cdots,a_{n}$の$M$個の``独立な''事象であるとすると、情報源全体の情報量は
\begin{align*}
H & =-P\left(a_{1}\right)\log P\left(a_{1}\right)-P\left(a_{2}\right)\log P\left(a_{2}\right)\cdots\\
 & =-\sum_{i=1}^{M}P\left(a_{i}\right)\log P\left(a_{i}\right)
\end{align*}

\begin{itemize}
\item 独立な$M$個であれば、平均情報量が情報源のエントロピー(正の情報量)
\item 独立でないとき?

\begin{itemize}
\item →シンボルの$m$次拡大
\item →拡大した情報源に対して平均情報源
\end{itemize}
\end{itemize}

\paragraph{定義 情報源の情報量}

複数の事象の集まりに対する情報量
\begin{center}
\includegraphics{images/InformationScience/1-4}
\par\end{center}

\begin{align*}
H\left(A_{1}\right) & =-0.8\log0.8-0.15\log0.15-0.05\log0.05\\
 & =0.88\left(\text{ビット}\right)\\
H\left(A_{2}\right) & =1.58\left(\text{ビット}\right)
\end{align*}


\paragraph{エントロピーの性質}
\begin{enumerate}
\item 偏りが大きいとエントロピーが小さくなり、偏りが大きいとエントロピーは小さくなる。
\item $H\left(A\right)\geqq0$
\item $H\left(A\right)\leqq-\sum_{t=1}^{M}\frac{1}{M}\log\frac{1}{M}=\log M$
\end{enumerate}
\[
\because\begin{cases}
H=-\sum P_{i}\log P_{i}\\
\sum P_{i}=1
\end{cases}
\]
を条件に最大化し、
\[
L=H-\lambda\left(\sum P_{i}-1\right)
\]
\[
\begin{cases}
\pd L{P_{1}}=\pd L{P_{2}}=\cdots=0\\
\pd L{\lambda}=0
\end{cases}
\]
\[
\pd L{P_{i}}=-\log P_{i}-\log\e-\lambda=0
\]
\[
\pd L{\lambda}=\sum P_{i}-1=0
\]

\begin{align*}
\log P_{i} & =-\log\e-\lambda\\
P_{i} & =\frac{1}{\e2^{\lambda}}\\
 & =c
\end{align*}
とすると、
\[
c=\frac{1}{M}
\]


\paragraph{エントロピー関数}
\begin{center}
\includegraphics{images/InformationScience/1-5}
\par\end{center}

\[
H=-p\log p-\left(1-p\right)\log\left(1-p\right)
\]

\begin{center}
\includegraphics{images/InformationScience/1-6}
\par\end{center}

\paragraph{曖昧さとエントロピー}

エントロピー 曖昧さの変化の度合い
\begin{itemize}
\item あいまい
\item →シグナルを受けた時点で曖昧さが消える
\item →あいまい
\end{itemize}

\section*{第2回}

\paragraph{情報量}
\begin{itemize}
\item 自己情報量 $-\log p_{i}$: でき事
\item 平均情報量 $-\sum p_{i}\log p_{i}$: 情報源

\begin{itemize}
\item 無記憶情報源
\end{itemize}
\item あいまいさとエントロピー
\end{itemize}

\paragraph{エントロピーと代表的系列}
\begin{center}
\includegraphics{images/InformationScience/2-1}
\par\end{center}

$n$が十分大とすると、
\begin{align*}
\frac{n_{i}}{n} & \simeq p_{i}\\
n_{i} & \simeq np_{i}
\end{align*}

ある系列$\sigma$に対し、
\begin{align*}
P\left(\sigma\right) & =p^{n_{1}}p^{n_{2}}\cdots p^{n_{M}}\\
 & =\prod_{i=1}^{M}p^{n_{i}}\\
 & =\prod p^{np_{i}}\\
p_{i} & =2^{\log p_{i}}\\
P\left(\sigma\right) & =\prod2^{np_{i}\log p_{i}}\\
 & =2^{n\sum p_{i}\log p_{i}}\\
 & =2^{-nH\left(A\right)}\\
 & =\frac{1}{2^{nH\left(A\right)}}
\end{align*}


\paragraph{相互情報量}

・通信的立場
\begin{center}
\includegraphics{images/InformationScience/2-3}
\par\end{center}

通信による情報源の情報量\\
=$H\left(A\right)$－誤りによる混乱\\
=(情報を受け取る以前の曖昧さ)－(通信を受け取った以降に残る曖昧さ=誤りに対する減少分)

・汎用的立場

複合事象のエントロピー
\begin{center}
\includegraphics{images/InformationScience/2-4}
\par\end{center}

$B$を観測→どれだけ$A$の情報量を受け取ることができるか

$B$を介して知ることのできる情報源の情報量

\paragraph{例}
\begin{center}
\includegraphics{images/InformationScience/2-5}
\par\end{center}

\begin{align*}
H\left(A\right) & =-0.65\log0.65-0.35\log0.35\\
 & =0.93\\
H\left(B\right) & =-0.60\log0.60-0.4\log0.4\\
 & =0.97\\
H\left(AB\right) & =-\sum P\left(a_{i}b_{i}\right)\log P\left(a_{i}b_{i}\right)\\
 & =-0.55\log0.55\cdots-0.30\log0.30\\
 & =1.54
\end{align*}

\[
H\left(A\right),H\left(B\right)<H\left(AB\right)<H\left(A\right)+H\left(B\right)
\]

$B$を知ってのちの$A$の曖昧さ(条件付きエントロピー)

\begin{align*}
 & H\left(A|b_{1}\right)\\
 & =-\sum P\left(a_{i}|b_{i}\right)\log P\left(a_{i}|b_{i}\right)\\
 & =-\frac{55}{60}\log\frac{55}{60}-\frac{5}{60}\log\frac{5}{60}\\
 & =0.41
\end{align*}

\[
H\left(A|b_{2}\right)=\cdots-0.81
\]

\begin{align*}
H\left(A|B\right) & =-\sum P\left(b_{i}\right)H\left(A|b_{j}\right)\\
 & =-\sum_{i}P\left(b_{i}\right)\sum P\left(a_{j}|b_{i}\right)\log P\left(a_{j}b_{i}\right)\\
 & =0.57
\end{align*}

\[
I\left(A,B\right)=H\left(A\right)-H\left(A|B\right)=0.36
\]

$I\left(A,B\right)$: $B$を知ることで分かる$A$の情報量 相互情報量

\begin{align*}
I\left(A,B\right)= & -\sum_{i}P\left(a_{i}\right)\log P\left(a_{i}\right)\\
 & +\sum_{i}P\left(b_{j}\right)\sum_{i}P\left(a_{i}|b_{j}\right)\log P\left(a_{i}|b_{j}\right)\\
= & \sum_{i}\sum_{j}P\left(a_{i}b_{j}\right)\log\frac{P\left(a_{i}|b_{j}\right)}{P\left(a_{i}\right)P\left(b_{j}\right)}
\end{align*}

$B$から得る$A$の情報量=$A$から得る$B$の情報量
\begin{center}
\includegraphics{images/InformationScience/2-6}
\par\end{center}

\paragraph{相互情報量の性質}

\[
0\leq I\left(A,B\right)\leq H\left(A\right)
\]

\[
0\leq H\left(A,B\right)\leq H\left(A\right)
\]


\paragraph{例: 天気予報}
\begin{center}
\includegraphics{images/InformationScience/2-7}
\par\end{center}

\[
H\left(A\right)=1\mathrm{bit}
\]

\begin{align*}
I\left(A,B\right) & =\sum_{i}\sum_{j}P\left(a_{i}b_{j}\right)\log\frac{P\left(a_{i}b_{j}\right)}{P\left(a_{i}\right)P\left(b_{j}\right)}\\
 & =1+p\log p+\left(1-p\right)\log\left(1-p\right)
\end{align*}

あるいは
\begin{align*}
H\left(A|B\right) & =\underbrace{-p\log p-\left(1-p\right)\log\left(1-p\right)}_{\text{エントロピー関数}}\\
I\left(A,B\right) & =H\left(A\right)-H\left(A|B\right)
\end{align*}

\begin{center}
\includegraphics{images/InformationScience/2-8}
\par\end{center}

\begin{align*}
I\left(A,B\right) & =H\left(A\right)-H\left(A|B\right)\\
 & =H\left(B\right)-H\left(B|A\right)\\
 & =H\left(A\right)+H\left(B\right)-H\left(AB\right)
\end{align*}

\begin{center}
\includegraphics{images/InformationScience/2-9}
\par\end{center}

\section*{第3回}

\paragraph{相互情報量}

情報源符号化(情報圧縮)
\begin{itemize}
\item 符号の望ましい性質
\item どのように符号化?
\item どこまで圧縮できるか?
\end{itemize}

\paragraph{望ましい性質}

例:
\begin{center}
\includegraphics{images/InformationScience/3-1}
\par\end{center}

平均符号長 $L=\sum p_{i}l_{i}$

$C_{5}$:
\begin{center}
\includegraphics{images/InformationScience/3-2}
\par\end{center}
\begin{itemize}
\item 一意復号可能
\begin{itemize}
\item 瞬時符号
\begin{itemize}
\item 等長符号
\item 非等長符号
\end{itemize}
\item 非瞬時符号
\end{itemize}
\item 一意復号不可能 ($C_{5}$)
\end{itemize}
\begin{center}
\includegraphics{images/InformationScience/3-3}
\par\end{center}

\paragraph{瞬時符号の条件}

語頭条件 (Prefix rule): ある符号が別の符号の語頭になっていない (必要十分条件)

\paragraph{符号の木}
\begin{center}
\includegraphics{images/InformationScience/3-4}
\par\end{center}

瞬時符号→符号の木の葉にだけ符号がある

\paragraph{クラフトの不等式}

符号の``長さ''に関する条件

長さ$l_{1},l_{2},\cdots,l_{m}$となる瞬時符号が存在する必要十分条件

\[
2^{-l_{1}}+2^{-l_{2}}+\cdots+2^{-l_{m}}\le1
\]


\paragraph{平均符号長の条件}

どこまで短くできるか?

\paragraph{情報源符号化定理}

平均符号長$L$は、情報源のエントロピー$H\left(S\right)$に限りなく近づけることができる。

→
\[
H\left(S\right)\le L\le H\left(S\right)+\varepsilon
\]

\begin{center}
\includegraphics{images/InformationScience/3-5}
\par\end{center}

\[
H\left(S\right)=-\sum p_{i}\log p_{i}
\]

\[
L=\sum p_{i}l_{i}
\]


\paragraph{定理}
\begin{enumerate}
\item $L<H\left(S\right)+1$となる瞬時符号を作ることができる。
\item $H\left(S\right)\le L$でなければならない
\end{enumerate}
→$H\left(S\right)\le L<H\left(S\right)+1$

\paragraph{1. について}

符号長$l_{i}$について
\[
-\log p_{i}\le l_{i}<-\log_{i}+1
\]
と$l_{i}$を選ぶ。

\[
\log p_{i}\ge-l_{i}
\]

\[
p_{i}\ge2^{-l_{i}}
\]

\[
\sum p_{i}\ge\sum2^{-l_{i}}
\]

\[
1\ge\sum2^{-l_{i}}
\]

クラフトの不等式

瞬時符号が存在する。

\paragraph{シャノンの補助定理}

$q_{1},\cdots'q_{m}$を非負の数で$q_{1}+\cdots+q_{m}=1$とする。

\[
-p_{i}\log q_{i}\ge-p_{i}\log p_{i}
\]

等号は$q_{i}=p_{i}$のとき成立(シャノンの補助定理)。

$l_{i}=-\log q_{i}$と$q_{i}$を選ぶ。

$l_{i}$瞬時符号があれば
\[
\sum2^{-l_{i}}\le1
\]


\paragraph{どうやって最短符号を作るか?}

ハフマン府外: 平均符号長を最短化する符号
\begin{center}
\includegraphics{images/InformationScience/3-6}
\par\end{center}
\end{document}
